# Basic configuration for our connector
name=sink-postgres-customer-distributed
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
# We could parallel this task to increase performance
tasks.max=1

# Topic to be consumed
topics=dbserver1.inventory.customers,dbserver1.inventory.products,dbserver1.inventory.addresses

# Transformations
transforms=unwrap,changeTopicName,replaceField,castDelete
# Simplify Debezium Record extracting just the after field
transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState
# Include the delete information into record's value
transforms.unwrap.delete.handling.mode=rewrite
# Modifies Record's Topic name getting just the string value after the last dot(.)
# In order to mysql use it to create the table
transforms.changeTopicName.type=org.apache.kafka.connect.transforms.RegexRouter
transforms.changeTopicName.regex=dbserver1.inventory.(.*)
transforms.changeTopicName.replacement=$1
# Modify the record value field from __deleted to delete
transforms.replaceField.type=org.apache.kafka.connect.transforms.ReplaceField$Value
transforms.replaceField.renames=__deleted:deleted
# Modify the record value type from string to boolean
transforms.castDelete.type=org.apache.kafka.connect.transforms.Cast$Value
transforms.castDelete.spec=deleted:boolean

# Postgres configuration
connection.url=jdbc:postgresql://postgres:5432/postgres
connection.user=postgres
connection.password=postgres
dialect.name=PostgreSqlDatabaseDialect
# Perform Upsert to add records into the table
insert.mode=upsert
# Use the record's key value to create primary key
pk.mode=record_key
pk.fields=id
# Auto Create Table if not exists
auto.create=true
# Apply Alter table if a new column is detected
auto.evolve=true
table.name.format=${topic}