# Basic configuration for our connector
name=sink-azure-blob-connector
connector.class=io.confluent.connect.azure.blob.AzureBlobStorageSinkConnector
# We could parallel this task to increase performance
tasks.max=1

# Topic to be consumed
topics=dbserver1.inventory.customers,dbserver1.inventory.products,dbserver1.inventory.addresses
# Transformations
transforms=unwrap,renameField
# Simplify Debezium Record extracting just the after field
transforms.unwrap.type=io.debezium.transforms.ExtractNewRecordState
# Include the delete information into record's value
transforms.unwrap.delete.handling.mode=rewrite
# Include timestamp on extract record
transforms.unwrap.add.fields=ts_ms
# Rename field __ts_ms to timestamp, __deleted to delete
transforms.renameField.type=org.apache.kafka.connect.transforms.ReplaceField$Value
transforms.renameField.renames=__ts_ms:timestamp,__deleted:deleted


# Azure Blob configuration
azblob.account.name=kedablobstorage
azblob.account.key=fxfhlrUi50rlJpyB/3vXJs9jg8DlVr3Hn4hzeW8ITFDD203evXqIgWg2/TBRuPaTziPxGPzmGg9/+AStl9iR9A==
azblob.container.name=kafka-connect
format.class=io.confluent.connect.azure.blob.format.json.JsonFormat
topics.dir=data
directory.delim=/
file.delim=+
partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
path.format=YYYY/MM/dd/HH/mm
partition.duration.ms=600000
locale=pt_BR
timezone=UTC
timestamp.extractor=RecordField
timestamp.field=timestamp
flush.size=3
rotate.interval.ms=60000

# License
confluent.license=
confluent.topic=_confluent-command
confluent.topic.replication.factor=1
confluent.topic.bootstrap.servers=kafka-cluster:9092